{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Subscribe** to this channel and **follow** *@dsbyhadi* on twitter for updates!\n",
    "\n",
    "*email: datasciencebyhadi@gmail.com*\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deals with `unstructured` data(e.g. text) vs. `structured` data (e.g. tabular data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's *natural* language --> there are a variety of problems in this area.\n",
    "\n",
    "* Named Entity Recognition\n",
    "* Part Of Speech (POS) tagging\n",
    "* Machine Translation\n",
    "* Text Summarization \n",
    "* Sentiment Analysis\n",
    "* ...\n",
    "\n",
    "(of course `you don't need to be a lingustic expert!`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's an example of Text Classification Problem and by this we are going to cover:\n",
    "\n",
    "* Binary Classificaiton (The document is + or -)\n",
    "* Multi-Class Classification (The document is + or neutral or -) or ( The document is ++ or + or . or - or --)\n",
    "* Multi-label Classification (The document has profanity or hate speech or insult, i.e. each document could have multiple labels at the same time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches\n",
    "* Bag of Words (BOW)\n",
    "* Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Packages/Tools **\n",
    "* sklearn \n",
    "* spaCy\n",
    "* NLTK\n",
    "* StanfordNLP\n",
    "* spark NLP (John Snow Labs)\n",
    "* FastText\n",
    "* .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  \n",
    "  \n",
    "    \n",
    "    \n",
    "      \n",
    "         \n",
    "         \n",
    "            \n",
    "              \n",
    "                \n",
    "                  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW\n",
    "\n",
    "Tokens(e.g. words) are the features, and a process similar to one-hot encoding happens on them. Then a document is represented by the sum of vectors corresponding to its tokens.\n",
    "\n",
    "Example.\n",
    "```\n",
    "Corpus:\n",
    "Doc1. Dog is playing.     \n",
    "Doc2. I love my dog.      \n",
    "\n",
    "features (One-hot):\n",
    "dog =     (1, 0, 0, 0, 0, 0)\n",
    "is =      (0, 1, 0, 0, 0, 0)\n",
    "playing = (0, 0, 1, 0, 0, 0)\n",
    "i =       (0, 0, 0, 1, 0, 0)\n",
    "love =    (0, 0, 0, 0, 1, 0)\n",
    "my =      (0, 0, 0, 0, 0, 1)\n",
    "\n",
    "So \n",
    "Doc1 --> (1, 1, 1, 0, 0, 0)\n",
    "Doc2 --> (1, 0, 0, 1, 1, 1)\n",
    "```\n",
    "- Some forms of normalizations are used (sometimes to reduce the space dimension)\n",
    " * tf\n",
    " * tf-idf\n",
    " * lemmatization/stemming (am, are, is $\\Rightarrow$ be) (cat, cats, cat's $\\Rightarrow$ cat)\n",
    " * lower-case\n",
    " * removing stop words (e.g. 'but', 'and', 'a', 'i', 'me', 'that', ...)\n",
    " * .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### More on TF-IDF\n",
    "\n",
    "Doc1: Dog is playing.\n",
    "\n",
    "|  term    | count |\n",
    "|----------|-------|\n",
    "|  dog     |  1    |\n",
    "|  is      |  1    |\n",
    "|  playing |  1    |\n",
    "\n",
    "Doc2: I love my dog.\n",
    "\n",
    "|  term    | count |\n",
    "|----------|-------|\n",
    "|  i       |  1    |\n",
    "|  love    |  1    |\n",
    "|  my      |  1    |\n",
    "|  dog     |  1    |\n",
    "\n",
    "$tf(w, d) := f_{w,d}\\div |d|$ where $f_{w,d}$ is the frequency of $w$ in $d$, and $|d|$ is the total number of words (tokens) in document $d$.   \n",
    "$idf(w) := log(\\frac{N}{t_{w}})$ where $t_w$ is the number of documents that contain $w$, and $N$ is the total number of documents in the corpus, and finally  \n",
    "$tf{\\text -}idf(w, d) = tf(w, d) \\times idf(w)$  \n",
    "(Note: there are many variations of these functions, and these are some simple ones used for this example to relay the idea behind them)\n",
    "\n",
    "$tf(\"dog\", d_1) = \\frac{1}{3} = 0.33$  \n",
    "$tf(\"playing\", d_1) = \\frac{1}{3} = 0.33$   \n",
    "$tf(\"dog\", d_2) = \\frac{1}{4} = 0.25$  \n",
    "..   \n",
    "\n",
    "$idf(\"dog\") = log(\\frac{2}{2}) = 0$  \n",
    "$idf(\"playing\") = log(\\frac{2}{1}) = 1$  \n",
    "..  \n",
    "\n",
    "Hence   \n",
    "$tf{\\text -}idf(\"dog\", d_1) = 0.33 \\times 0 = 0$,  \n",
    "$tf{\\text -}idf(\"dog\", d_2) = 0.25 \\times 0 = 0$,  \n",
    "$tf{\\text -}idf(\"playing\", d_1) = 0.33 \\times 1 = 0.33$  \n",
    "..   \n",
    "\n",
    "and so,  \n",
    "```Doc1 --> (0, 0.33, 0.33, 0, 0, 0)  \n",
    "Doc2 --> (0, 0, 0, 0.25, 0.25, 0.25)  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding\n",
    "\n",
    "Uses a bottleneck structure to reduce the features space dimension of BOW.\n",
    "\n",
    "Example:\n",
    "```\n",
    "Corpus:\n",
    "Doc1. Dog is playing.\n",
    "Doc2. I love my dog.\n",
    "\n",
    "embeddings (dimension 2):\n",
    "dog =     (2, 4)\n",
    "is =      (1, -1)\n",
    "playing = (3, 2)\n",
    "i =       (-1, -2)\n",
    "love =    (-1, 10)\n",
    "my =      (0, 1)\n",
    "\n",
    "So \n",
    "Doc1 --> (6, 5)\n",
    "Doc2 --> (0, 13)\n",
    "```\n",
    "- features dimension reduction (way smaller space compared to BOW word vectors)\n",
    "- Word vectors are passed through a bottleneck structure (neural network with one hidden layer)\n",
    " \n",
    " ![title](embedding_sketch_1.jpeg)\n",
    " \n",
    " * Other possible approaches exist e.g. matrix factorization, collaborative filtering, etc.\n",
    "\n",
    "- Example embeddings: word2vec, GloVe, BERT, ...\n",
    "- Makes use of semantic and syntactic i.e. words that are close in the meaning or usage, are close in the embedding space as well\n",
    " * As long as words with the same meaning have their vectors close to each other (kinda clustered around each other), the canonical phenomena naturally happen \n",
    "   * e.g. vec(“Paris”) - vec(“France”) + vec(“Germany”) = Vec(“Berlin”) or vec(“Tehran”) - vec(“Iran”) + vec(“Germany”) = Vec(“Berlin”)\n",
    " * Potential issue? words could have different meanings, e.g. ‘point’, 'leaves' → other embeddings such as BERT have tried to resolve this issue\n",
    "- CBOW (given context words, predict target word) vs. Skipgram (given target word, predict contex words)\n",
    "- fasttext works at char n-gram level vs. word2vec or glove that work at word level\n",
    "- There are plenty of pre-trained embeddings that can be reused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My suggestion is to try both approaches on your specific problem. No preference is given to any of them, but the only advice is to ***start simple***, and then run more interations, change modules/parameters/approach/etc. to improve the model over time.\n",
    "\n",
    "Keep in mind, your training data is very critical, bad training data (small data, inconsistent labels, uncleaned text) results in bad models, no matter what top-notch technique you use! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  \n",
    "  \n",
    "    \n",
    "    \n",
    "      \n",
    "         \n",
    "         \n",
    "            \n",
    "              \n",
    "                \n",
    "                  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges:\n",
    "\n",
    "- Good enough (realistic) vs. Best (Kaggle)\n",
    "- Not enough training data\n",
    " * Use augmentation to increase data and performance\n",
    "- Skewed training data\n",
    "- Normalizing text depending on the source\n",
    " * Twitter's text data --> abbreviations/hashtags/etc.    \n",
    " * SMS Text --> smileys/Capital case matters.\n",
    "- Encountering other languages --> remove them/translate them\n",
    "- Converging to a good set of metrics\n",
    " * eyeball some examples of the test set at the end\n",
    "- Deploying to production\n",
    " * flask REST API\n",
    " * Hand over to engineers\n",
    " * SageMaker, etc.\n",
    " * MLeap\n",
    " * ..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remember: \n",
    "All models are wrong, some are useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
