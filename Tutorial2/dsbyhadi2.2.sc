%md

### Gradient Boosting Tree Classification with
### Cross Validation Tuning in Scala using Spark ML

*Hadi*

%md

- Implement GBT classifier
- Evaluate the GBT classifier
  - Tune and find best parameters using Cross Validation

%md

<b>Boosting</b>:
  - based on weak learners (high bias, low variance) e.g. shallow decision trees --> GBT
- learners(trees) are trained serially

<b>Random Forest</b>:
  - based on large decision trees (low bias, high variance)
- trees are trained in parallel

%md

### Implementing GBT classifier

import org.apache.spark.sql.DataFrame

val data = spark
  .read
  .format("org.apache.spark.csv")
  .option("inferSchema", "true")
  .option("header", "true")
  .csv("/Users/hadi.minooei/Documents/DSbyHadi/car_ownership.csv")

data.count
data.columns
data.printSchema

// We need to predict owns_car --> response
// Features:
// 'income' is a continuous feature
// 'state' and 'gender' are categorical features

// We need to index the owns_car i.e. true -> 0.0 and false -> 1.0 or vice versa.
// For this we use StringIndexer.

import org.apache.spark.sql.types.StringType

val dataDF = data
  .withColumn("owns_car_string", data("owns_car").cast(StringType))
  .drop("owns_car")
  .withColumnRenamed("owns_car_string", "owns_car")

import org.apache.spark.ml.feature.{StringIndexer}

val labelIndexer = new StringIndexer()
  .setInputCol("owns_car")
  .setOutputCol("owns_car_index")
  .fit(dataDF)

labelIndexer.transform(dataDF).columns
labelIndexer.transform(dataDF).take(4)

//categorical features ('state' and 'gender') need to be indexed.

%md
  ##### Do not need to do OneHotEncoding

val genderIndexer = new StringIndexer()
  .setInputCol("gender")
  .setOutputCol("gender" + "_index")
  .setHandleInvalid("skip")
  .fit(dataDF)

val stateIndexer = new StringIndexer()
  .setInputCol("state")
  .setOutputCol("state" + "_index")
  .setHandleInvalid("skip")
  .fit(dataDF)

val test1 = stateIndexer.transform(dataDF)
val test2 = genderIndexer.transform(test1)

test2.columns
test2.take(6)

// Putting all featurs together in a vector

// features: state_index, gender_index, income

%md
  <b>VectorAssembler</b> is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector

import org.apache.spark.ml.feature.VectorAssembler

val featuresAssembler = new VectorAssembler()
  .setInputCols(Array("income", "state_index", "gender_index"))
  .setOutputCol("features")

val assembled = featuresAssembler.transform(test2)
assembled.columns
assembled.head

%md

##### No need to normalize continuous features.

// Define the Gradient Boosting Tree model

import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}

val gbt = new GBTClassifier()
  .setLabelCol("owns_car_index")
  .setFeaturesCol("features")
  .setMaxIter(4) // number of trees, default is 20
  .setMaxDepth(3) // default is 5 (the same as Random Forest)

%md
  #### Note: Multiclass labels are not _currently_ supported. (Spark doc)

%md

<b>Parameters from TreeClassifierParams:</b>
setMaxDepth
setMaxBins
setMinInstancesPerNode
setMinInfoGain
setMaxMemoryInMB
setCacheNodeIds
setCheckpointInterval

<b>Parameters from TreeEnsembleParams:</b>
setSubsamplingRate
setSeed

<b>Parameters from GBTParams:</b>
setMaxIter
setStepSize //i.e. Learning rate

<b>Parameters from GBTClassifierParams:</b>
setLossType //Supported: "logistic"

<b>You can check the definitions and default values in here:</b>
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/tree/treeParams.scala

// Now we put the pipeline together

import org.apache.spark.ml.{Pipeline, PipelineModel}

val pipeline = new Pipeline().setStages(Array(genderIndexer, stateIndexer) ++ Array(featuresAssembler, labelIndexer, gbt))

// We do train-test split first.
val Array(trainDF, testDF) = dataDF.randomSplit(Array(0.65, 0.35))
trainDF.count
testDF.count

val model = pipeline.fit(trainDF)

// We can now make predictions using 'model'

val testPreds = model.transform(testDF)
testPreds.columns
testPreds.select("owns_car_index", "prediction").take(3)

labelIndexer.labels
// so true mapped to index 0.0
// false mapped to index 1.0

%md

#### Evaluate the GBT Classifier

// Can compute AUCROC similar to logistic regression since we have two classes here. (see Tutorial 1)
// Also other metrics such as "f1", "weightedPrecision", "weightedRecall", "accuracy" are available (see Tutorial 2.1)

import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator}

val evaluator = new MulticlassClassificationEvaluator()
  .setLabelCol("owns_car_index")
  .setPredictionCol("prediction")
  .setMetricName("f1")

evaluator.evaluate(testPreds)

%md

#### Tune and find best parameters using Cross Validation

import org.apache.spark.sql.DataFrame
import org.apache.spark.ml.Model
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler};
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}
import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}


val IndexerPostfix = "_index"
val FeaturesVectorColumn = "features"
val ActionForInvalidFlag = "skip" // Can also be "keep" or "error"
val TrainTestSplitRatio = 0.65

def trainGBTModel(
    allData: DataFrame,
    labelCol: String,
    continuousFeatures: Array[String],
    categoricalFeatures: Array[String]): Model[_] = {

  val labelIndexer = new StringIndexer()
    .setInputCol(labelCol)
    .setOutputCol(labelCol + IndexerPostfix)
    .fit(allData)

  // Indexing categorical features
  val indexer = (featureName: String) => Seq(
    new StringIndexer()
      .setInputCol(featureName)
      .setOutputCol(featureName + IndexerPostfix)
      .setHandleInvalid(ActionForInvalidFlag)
      .fit(allData))

  val categoricalStages = categoricalFeatures.flatMap(indexer)

  val featuresNames = continuousFeatures ++ categoricalFeatures.map(_ + IndexerPostfix)

  val featuresAssembler = new VectorAssembler()
    .setInputCols(featuresNames.toArray)
    .setOutputCol(FeaturesVectorColumn)

  val gbt = new GBTClassifier()
    .setLabelCol("owns_car_index")
    .setFeaturesCol("features")
  //.setMaxIter(4) // will add them in CV
  //.setMaxDepth(3) // will add them in CV

  val evaluator = new MulticlassClassificationEvaluator()
    .setLabelCol("owns_car_index")
    .setPredictionCol("prediction")
    .setMetricName("f1")

  val paramGrid = new ParamGridBuilder()
    .addGrid(gbt.maxIter, Array(3, 4, 10))
    .addGrid(gbt.maxDepth, Array(2, 3))
    .build()

  val pipeline = new Pipeline().setStages(categoricalStages ++ Array(featuresAssembler, labelIndexer, gbt))

  //val Array(trainDF, testDF) = allData.randomSplit(Array(TrainTestSplitRatio, 1 - TrainTestSplitRatio))

  val cv = new CrossValidator()
    .setEstimator(pipeline)
    .setEvaluator(evaluator)
    .setEstimatorParamMaps(paramGrid)
    .setNumFolds(3) // At least 2

  val fittedCV = cv.fit(allData)
  //...
  fittedCV.write.overwrite().save("/Users/hadi.minooei/Documents/DSbyHadi/gbt_cv/") // User  CrossValidatorModel.load("modelOutput") to reload and use the model.
  //...
  fittedCV
}

val trainedGBTModel = trainGBTModel(dataDF, "owns_car", Array("income"), Array("state", "gender"))
val preds = trainedGBTModel.transform(dataDF)
preds.columns
preds.head

%md
  ###### Let's check the best params chosen

val cvModel = CrossValidatorModel.load("/Users/hadi.minooei/Documents/DSbyHadi/gbt_cv/")

import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}

val bpm = cvModel.asInstanceOf[CrossValidatorModel].bestModel.asInstanceOf[PipelineModel]
val gbtModel = bpm.stages(bpm.stages.length - 1).asInstanceOf[GBTClassificationModel]
gbtModel.getMaxIter
gbtModel.getMaxDepth

