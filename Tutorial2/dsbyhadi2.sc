%md

### Random Forest Classification in Scala using Spark ML

*Hadi*

%md

#### Recap:
<b> Spark ML</b>: Works with DataFrames (vs. Spark Mllib works with RDDs)

<b>DataFrame</b>: This ML API uses DataFrame from Spark SQL as an ML dataset, which can hold a variety of data types. E.g., a DataFrame could have different columns storing text, feature vectors, true labels, and predictions.

<b>Transformer</b>: A Transformer is an algorithm which can transform one DataFrame into another DataFrame. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions.

<b>Estimator</b>: An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model.

<b>Pipeline</b>: A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.

(from spark.apache.org)

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.types._

val schemaStruct =
  StructType(
    StructField("state", StringType, false) ::
      StructField("gender", StringType, false) ::
      StructField("income", DoubleType, false) ::
      StructField("owns_car", BooleanType, true) :: Nil)

val data = spark
  .read
  .schema(schemaStruct)
  .option("header", "true")
  .csv("/Users/hadi.minooei/Documents/DSbyHadi/car_ownership.csv")

data.count
data.columns
data.printSchema

// We need to predict owns_car --> response
// Three features to be used: state, gender and income
// 'income' is a continuous feature
// 'state' and 'gender' are categorical features

// We need to index the owns_car i.e. true -> 0.0 and false -> 1.0 or vice versa.
// For this we use StringIndexer.

import org.apache.spark.ml.feature.{StringIndexer}

val labelIndexer = new StringIndexer()
  .setInputCol("owns_car")
  .setOutputCol("owns_car_index")
  .fit(dataDF)

labelIndexer.transform(dataDF).columns
labelIndexer.transform(dataDF).take(4)

// 'income' is a continuous feature

// 'state' and 'gender' are categorical features, so need to be indexed.

%md
  ##### No need to do OneHotEncoding

import org.apache.spark.ml.feature.OneHotEncoder;

val genderIndexer = new StringIndexer()
  .setInputCol("gender")
  .setOutputCol("gender" + "_index")
  .setHandleInvalid("skip")
  .fit(dataDF)

val stateIndexer = new StringIndexer()
  .setInputCol("state")
  .setOutputCol("state" + "_index")
  .setHandleInvalid("skip")
  .fit(dataDF)

val test1 = stateIndexer.transform(dataDF)
val test2 = genderIndexer.transform(test1)

test2.columns
test2.take(6)

// Putting all featurs together in a vector

// features: state_index, gender_index, income

%md
  <b>VectorAssembler</b> is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector

import org.apache.spark.ml.feature.VectorAssembler

val featuresAssembler = new VectorAssembler()
  .setInputCols(Array("income", "state_index", "gender_index"))
  .setOutputCol("features")

val assembled = featuresAssembler.transform(test2)
assembled.columns
assembled.head

%md

##### No need to normalize features. Some even suggest not doing that.

// Define the Random Forest model

import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}

val rf = new RandomForestClassifier()
  .setLabelCol("owns_car_index")
  .setFeaturesCol("features")
  .setNumTrees(4) // Default is 20
  .setMaxDepth(3) // Default is 5

%md

<b>Parameters from TreeClassifierParams:</b>
setMaxDepth
setMaxBins
setMinInstancesPerNode
setMinInfoGain
setMaxMemoryInMB
setCacheNodeIds
setCheckpointInterval
setImpurity

<b>Parameters from TreeEnsembleParams:</b>
setSubsamplingRate
setSeed

<b>Parameters from RandomForestParams:</b>
setNumTrees
setFeatureSubsetStrategy

<b>You can check the definitions and default values in here:</b>
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/tree/treeParams.scala

// Now we put the pipeline together

import org.apache.spark.ml.{Pipeline, PipelineModel}

val pipeline = new Pipeline().setStages(Array(genderIndexer, stateIndexer) ++ Array(featuresAssembler, labelIndexer, rf))

// We do train-test split first.
val Array(trainDF, testDF) = dataDF.randomSplit(Array(0.65, 0.35))

trainDF.count
testDF.count

val model = pipeline.fit(trainDF)

// We can now make predictions using 'model'

val testPreds = model.transform(testDF)
testPreds.columns
testPreds.select("owns_car_index", "rawPrediction", "probability", "prediction").take(3)

labelIndexer.labels
// so true mapped to index 0.0
// false mapped to index 1.0

// Could compute AUCROC similar to logistic regression since we have two classes here. (see Tutorial 1)

import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator}

val evaluator = new MulticlassClassificationEvaluator()
  .setLabelCol("owns_car_index")
  .setPredictionCol("prediction")
  .setMetricName("accuracy") // metrics:"f1" (default), "weightedPrecision", "weightedRecall", "accuracy"

evaluator.evaluate(testPreds)

val trainPreds = model.transform(trainDF)
trainPreds.head
evaluator.evaluate(trainPreds)

import org.apache.spark.sql.DataFrame
import org.apache.spark.ml.Model
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler};
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}


val IndexerPostfix = "_index"
val FeaturesVectorColumn = "features"
val ActionForInvalidFlag = "skip" // Can also be "keep" or "error"
val TrainTestSplitRatio = 0.65

def trainRFModel(
    allData: DataFrame,
    labelCol: String,
    continuousFeatures: Array[String],
    categoricalFeatures: Array[String]): Model[_] = {

  val labelIndexer = new StringIndexer()
    .setInputCol(labelCol)
    .setOutputCol(labelCol + IndexerPostfix)
    .fit(allData)

  // Indexing categorical features
  val indexer = (featureName: String) => Seq(
    new StringIndexer()
      .setInputCol(featureName)
      .setOutputCol(featureName + IndexerPostfix)
      .setHandleInvalid(ActionForInvalidFlag)
      .fit(allData))

  val categoricalStages = categoricalFeatures.flatMap(indexer)

  val featuresNames = continuousFeatures ++ categoricalFeatures.map(_ + IndexerPostfix)

  val featuresAssembler = new VectorAssembler()
    .setInputCols(featuresNames.toArray)
    .setOutputCol(FeaturesVectorColumn)

  val rf = new RandomForestClassifier()
    .setLabelCol(labelCol + IndexerPostfix)
    .setFeaturesCol(FeaturesVectorColumn)
    .setNumTrees(4)
    .setMaxDepth(3)

  val pipeline = new Pipeline().setStages(categoricalStages ++ Array(featuresAssembler, labelIndexer, rf))

  val Array(trainDF, testDF) = allData.randomSplit(Array(TrainTestSplitRatio, 1 - TrainTestSplitRatio))

  val trainedModel = pipeline.fit(trainDF)

  // do other things like evaluation..

  trainedModel
}

dataDF.columns
val trainedRFModel = trainRFModel(dataDF, "owns_car", Array("income"), Array("state", "gender"))

val preds = trainedRFModel.transform(dataDF)
preds.columns
preds.head

// Next: We will show how to train a GBT model
