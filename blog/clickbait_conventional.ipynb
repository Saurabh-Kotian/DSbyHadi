{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we detect clickbait headlines using a Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is on Kaggle https://www.kaggle.com/amananandrai/clickbait-dataset however you can downloaded the file directly from the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.github.com/hminooei/DSbyHadi/master/data/clickbait_data.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Should I Get Bings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        headline  clickbait\n",
       "0                             Should I Get Bings          1\n",
       "1  Which TV Female Friend Group Do You Belong In          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>31998</td>\n",
       "      <td>Netanyahu Urges Pope Benedict, in Israel, to Denounce Iran</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31999</td>\n",
       "      <td>Computer Makers Prepare to Stake Bigger Claim in Phones</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         headline  clickbait\n",
       "31998  Netanyahu Urges Pope Benedict, in Israel, to Denounce Iran          0\n",
       "31999     Computer Makers Prepare to Stake Bigger Claim in Phones          0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "df.head(2)\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7facf3a60fd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "percentages = round(df[\"clickbait\"].value_counts()*100/len(df), 1)\n",
    "percentages.plot(kind=\"bar\", title=\"labels' distribution\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the labels are equaly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(pipeline, text_train, label_train, text_val, label_val):\n",
    "    train_preds = pipeline.predict(text_train)\n",
    "    val_preds = pipeline.predict(text_val)\n",
    "    \n",
    "    print(\"train:\")\n",
    "    print(metrics.classification_report(label_train, train_preds, labels=[0, 1], digits=4))\n",
    "    print(metrics.confusion_matrix(label_train, train_preds))\n",
    "    print(\"validation:\")\n",
    "    print(metrics.classification_report(label_val, val_preds, labels=[0, 1], digits=4))\n",
    "    print(metrics.confusion_matrix(label_val, val_preds))\n",
    "\n",
    "\n",
    "def train_measure_model(text_train, label_train, text_val, label_val,\n",
    "                        cv_binary, cv_analyzer, cv_ngram, cv_max_features,\n",
    "                        cv_have_tfidf, cv_use_idf, cfr_penalty, cfr_C, stop_words=None, \n",
    "                        text_column_name=\"headline\"):\n",
    "    cv = CountVectorizer(binary=cv_binary, stop_words=stop_words,\n",
    "                               analyzer=cv_analyzer,\n",
    "                               ngram_range=cv_ngram[1:3],\n",
    "                               max_features=cv_max_features)\n",
    "    if cv_have_tfidf:\n",
    "        pipeline = Pipeline(steps=[(\"vectorizer\", cv), \n",
    "                                   (\"tfidf\", TfidfTransformer(use_idf=cv_use_idf)),\n",
    "                                   (\"classifier\", LogisticRegression(penalty=cfr_penalty,\n",
    "                                                                     C=cfr_C,\n",
    "                                                                     random_state=9,\n",
    "                                                                     max_iter=100,\n",
    "                                                                     n_jobs=None))])\n",
    "    else:\n",
    "        pipeline = Pipeline(steps=[(\"vectorizer\", cv), \n",
    "                                   (\"classifier\", LogisticRegression(penalty=cfr_penalty,\n",
    "                                                                     C=cfr_C,\n",
    "                                                                     random_state=9,\n",
    "                                                                     max_iter=100,\n",
    "                                                                     n_jobs=None))])\n",
    "\n",
    "    pipeline.fit(text_train, label_train)\n",
    "    print_metrics(pipeline, text_train, label_train, text_val, label_val)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_val, text_test, label_train_val, label_test = train_test_split(\n",
    "    df[\"headline\"], \n",
    "    df[\"clickbait\"], \n",
    "    test_size=0.25, \n",
    "    stratify=df[\"clickbait\"], \n",
    "    random_state=9)\n",
    "\n",
    "# Split the train_val dataset to train and validation separete portions.\n",
    "text_train, text_val, label_train, label_val = train_test_split(\n",
    "    text_train_val,\n",
    "    label_train_val, \n",
    "    test_size=0.2, \n",
    "    random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19200,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(19200,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(4800,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(4800,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(8000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(8000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train.shape\n",
    "label_train.shape\n",
    "\n",
    "text_val.shape\n",
    "label_val.shape\n",
    "\n",
    "text_test.shape\n",
    "label_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9631    0.9814    0.9722      9627\n",
      "           1     0.9809    0.9622    0.9715      9573\n",
      "\n",
      "    accuracy                         0.9718     19200\n",
      "   macro avg     0.9720    0.9718    0.9718     19200\n",
      "weighted avg     0.9720    0.9718    0.9718     19200\n",
      "\n",
      "[[9448  179]\n",
      " [ 362 9211]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9501    0.9705    0.9602      2374\n",
      "           1     0.9705    0.9501    0.9602      2426\n",
      "\n",
      "    accuracy                         0.9602      4800\n",
      "   macro avg     0.9603    0.9603    0.9602      4800\n",
      "weighted avg     0.9604    0.9602    0.9602      4800\n",
      "\n",
      "[[2304   70]\n",
      " [ 121 2305]]\n",
      "CPU times: user 2.53 s, sys: 201 ms, total: 2.73 s\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cfr_pipeline = train_measure_model(text_train, label_train, text_val, label_val,\n",
    "                                   cv_binary=False, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                   cv_max_features=50000, cv_have_tfidf=True, cv_use_idf=False, \n",
    "                                   cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9615    0.9773    0.9693      9627\n",
      "           1     0.9767    0.9606    0.9686      9573\n",
      "\n",
      "    accuracy                         0.9690     19200\n",
      "   macro avg     0.9691    0.9689    0.9690     19200\n",
      "weighted avg     0.9691    0.9690    0.9690     19200\n",
      "\n",
      "[[9408  219]\n",
      " [ 377 9196]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9527    0.9760    0.9642      2374\n",
      "           1     0.9759    0.9526    0.9641      2426\n",
      "\n",
      "    accuracy                         0.9642      4800\n",
      "   macro avg     0.9643    0.9643    0.9642      4800\n",
      "weighted avg     0.9644    0.9642    0.9642      4800\n",
      "\n",
      "[[2317   57]\n",
      " [ 115 2311]]\n",
      "CPU times: user 1.32 s, sys: 23 ms, total: 1.34 s\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cfr_pipeline = train_measure_model(text_train, label_train, text_val, label_val,\n",
    "                                   cv_binary=False, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                   cv_max_features=5000, cv_have_tfidf=True, cv_use_idf=False, \n",
    "                                   cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9628    0.9780    0.9703      9627\n",
      "           1     0.9775    0.9620    0.9697      9573\n",
      "\n",
      "    accuracy                         0.9700     19200\n",
      "   macro avg     0.9701    0.9700    0.9700     19200\n",
      "weighted avg     0.9701    0.9700    0.9700     19200\n",
      "\n",
      "[[9415  212]\n",
      " [ 364 9209]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9547    0.9760    0.9652      2374\n",
      "           1     0.9760    0.9547    0.9652      2426\n",
      "\n",
      "    accuracy                         0.9652      4800\n",
      "   macro avg     0.9653    0.9653    0.9652      4800\n",
      "weighted avg     0.9654    0.9652    0.9652      4800\n",
      "\n",
      "[[2317   57]\n",
      " [ 110 2316]]\n",
      "CPU times: user 1.3 s, sys: 16.7 ms, total: 1.32 s\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cfr_pipeline = train_measure_model(text_train, label_train, text_val, label_val,\n",
    "                                   cv_binary=True, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                   cv_max_features=5000, cv_have_tfidf=True, cv_use_idf=False, \n",
    "                                   cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9712    0.9843    0.9777      9627\n",
      "           1     0.9840    0.9706    0.9773      9573\n",
      "\n",
      "    accuracy                         0.9775     19200\n",
      "   macro avg     0.9776    0.9775    0.9775     19200\n",
      "weighted avg     0.9776    0.9775    0.9775     19200\n",
      "\n",
      "[[9476  151]\n",
      " [ 281 9292]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9632    0.9815    0.9723      2374\n",
      "           1     0.9815    0.9633    0.9723      2426\n",
      "\n",
      "    accuracy                         0.9723      4800\n",
      "   macro avg     0.9724    0.9724    0.9723      4800\n",
      "weighted avg     0.9725    0.9723    0.9723      4800\n",
      "\n",
      "[[2330   44]\n",
      " [  89 2337]]\n",
      "CPU times: user 1.32 s, sys: 28.1 ms, total: 1.35 s\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cfr_pipeline = train_measure_model(text_train, label_train, text_val, label_val,\n",
    "                                   cv_binary=True, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                   cv_max_features=5000, cv_have_tfidf=True, cv_use_idf=True, \n",
    "                                   cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.3 ms, sys: 175 µs, total: 1.47 ms\n",
      "Wall time: 1.31 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time cfr_pipeline.predict(text_test[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_model_on_test(model, text_test, label_test):\n",
    "    test_preds = model.predict(text_test)\n",
    "    test_metrics = metrics.classification_report(label_test, test_preds, labels=[0, 1], digits=4)\n",
    "    print(test_metrics)\n",
    "    print(metrics.confusion_matrix(label_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9613    0.9690    0.9651      4000\n",
      "           1     0.9688    0.9610    0.9649      4000\n",
      "\n",
      "    accuracy                         0.9650      8000\n",
      "   macro avg     0.9650    0.9650    0.9650      8000\n",
      "weighted avg     0.9650    0.9650    0.9650      8000\n",
      "\n",
      "[[3876  124]\n",
      " [ 156 3844]]\n"
     ]
    }
   ],
   "source": [
    "measure_model_on_test(cfr_pipeline, text_test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the best model so far on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_name = 'clickbait-model-sm.pkl'\n",
    "cfr_pipeline.named_steps.vectorizer.stop_words_ = None # to reduce the pickle size (from 5mb to 500kb)\n",
    "pickle.dump(cfr_pipeline, open(model_name, 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find mislabeled samples by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incorrect_predictions(trained_model, all_data, text_df, label_series):\n",
    "    col_name = text_df.columns.values.tolist()[0]\n",
    "    preds = trained_model.predict(text_df[col_name])\n",
    "    incorrectly_predicted = text_df.loc[label_series != preds]\n",
    "    incorrectly_predicted.shape\n",
    "    res = incorrectly_predicted.merge(all_data, on=col_name, suffixes=(\"_left\", \"_right\"))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_added = get_incorrect_predictions(cfr_pipeline, df, text_train.to_frame(name=\"headline\"), label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Big Batch Breakfast Bakes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6 Little Victories</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Meet Stoner Sloth, The NSW Government's New Anti-Pot Mascot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      headline  clickbait\n",
       "0                                    Big Batch Breakfast Bakes          1\n",
       "1                                           6 Little Victories          1\n",
       "2  Meet Stoner Sloth, The NSW Government's New Anti-Pot Mascot          1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(432, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_be_added.head(3)\n",
    "to_be_added.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare and add the mislabeled samples back to the training set (**Manual Boosting**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_text_train = to_be_added[\"headline\"]\n",
    "extra_label_train = to_be_added[\"clickbait\"]\n",
    "\n",
    "extra_label_train = np.array(extra_label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9678    0.9778    0.9728      9778\n",
      "           1     0.9778    0.9677    0.9727      9854\n",
      "\n",
      "    accuracy                         0.9727     19632\n",
      "   macro avg     0.9728    0.9728    0.9727     19632\n",
      "weighted avg     0.9728    0.9727    0.9727     19632\n",
      "\n",
      "[[9561  217]\n",
      " [ 318 9536]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9652    0.9819    0.9735      2374\n",
      "           1     0.9820    0.9654    0.9736      2426\n",
      "\n",
      "    accuracy                         0.9735      4800\n",
      "   macro avg     0.9736    0.9736    0.9735      4800\n",
      "weighted avg     0.9737    0.9735    0.9735      4800\n",
      "\n",
      "[[2331   43]\n",
      " [  84 2342]]\n",
      "test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9648    0.9675    0.9662      4000\n",
      "           1     0.9674    0.9647    0.9661      4000\n",
      "\n",
      "    accuracy                         0.9661      8000\n",
      "   macro avg     0.9661    0.9661    0.9661      8000\n",
      "weighted avg     0.9661    0.9661    0.9661      8000\n",
      "\n",
      "[[3870  130]\n",
      " [ 141 3859]]\n"
     ]
    }
   ],
   "source": [
    "boosted_text_train = pd.concat([text_train, extra_text_train])\n",
    "boosted_label_train = np.concatenate([label_train, extra_label_train], axis=0)\n",
    "\n",
    "cfr_pipeline_1x = train_measure_model(boosted_text_train, boosted_label_train, \n",
    "                                      text_val, label_val,\n",
    "                                      cv_binary=True, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                      cv_max_features=5000, cv_have_tfidf=True, cv_use_idf=True, \n",
    "                                      cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)\n",
    "\n",
    "print(\"test:\")\n",
    "measure_model_on_test(cfr_pipeline_1x, text_test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boost the training set twice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9759    0.9799    0.9779      9929\n",
      "           1     0.9802    0.9763    0.9783     10135\n",
      "\n",
      "    accuracy                         0.9781     20064\n",
      "   macro avg     0.9781    0.9781    0.9781     20064\n",
      "weighted avg     0.9781    0.9781    0.9781     20064\n",
      "\n",
      "[[9729  200]\n",
      " [ 240 9895]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9656    0.9806    0.9730      2374\n",
      "           1     0.9807    0.9658    0.9732      2426\n",
      "\n",
      "    accuracy                         0.9731      4800\n",
      "   macro avg     0.9732    0.9732    0.9731      4800\n",
      "weighted avg     0.9732    0.9731    0.9731      4800\n",
      "\n",
      "[[2328   46]\n",
      " [  83 2343]]\n",
      "test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9672    0.9655    0.9663      4000\n",
      "           1     0.9656    0.9673    0.9664      4000\n",
      "\n",
      "    accuracy                         0.9664      8000\n",
      "   macro avg     0.9664    0.9664    0.9664      8000\n",
      "weighted avg     0.9664    0.9664    0.9664      8000\n",
      "\n",
      "[[3862  138]\n",
      " [ 131 3869]]\n"
     ]
    }
   ],
   "source": [
    "boosted_text_train_2x = pd.concat([text_train]+[extra_text_train]*2)\n",
    "boosted_label_train_2x = np.concatenate([label_train]+[extra_label_train]*2, axis=0)\n",
    "\n",
    "cfr_pipeline_2x = train_measure_model(boosted_text_train_2x, boosted_label_train_2x, \n",
    "                                      text_val, label_val,\n",
    "                                      cv_binary=True, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                      cv_max_features=5000, cv_have_tfidf=True, cv_use_idf=True, \n",
    "                                      cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)\n",
    "\n",
    "print(\"test:\")\n",
    "measure_model_on_test(cfr_pipeline_2x, text_test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the 2x manual boosted model has the best metrics. \n",
    "\n",
    "Below, you can see the effect of overfitting in the 5x version!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9802    0.9812    0.9807     10080\n",
      "           1     0.9818    0.9808    0.9813     10416\n",
      "\n",
      "    accuracy                         0.9810     20496\n",
      "   macro avg     0.9810    0.9810    0.9810     20496\n",
      "weighted avg     0.9810    0.9810    0.9810     20496\n",
      "\n",
      "[[ 9891   189]\n",
      " [  200 10216]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9659    0.9777    0.9717      2374\n",
      "           1     0.9779    0.9662    0.9720      2426\n",
      "\n",
      "    accuracy                         0.9719      4800\n",
      "   macro avg     0.9719    0.9719    0.9719      4800\n",
      "weighted avg     0.9719    0.9719    0.9719      4800\n",
      "\n",
      "[[2321   53]\n",
      " [  82 2344]]\n",
      "test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9681    0.9640    0.9661      4000\n",
      "           1     0.9642    0.9683    0.9662      4000\n",
      "\n",
      "    accuracy                         0.9661      8000\n",
      "   macro avg     0.9661    0.9661    0.9661      8000\n",
      "weighted avg     0.9661    0.9661    0.9661      8000\n",
      "\n",
      "[[3856  144]\n",
      " [ 127 3873]]\n"
     ]
    }
   ],
   "source": [
    "boosted_text_train_3x = pd.concat([text_train]+[extra_text_train]*3)\n",
    "boosted_label_train_3x = np.concatenate([label_train]+[extra_label_train]*3, axis=0)\n",
    "\n",
    "cfr_pipeline_3x = train_measure_model(boosted_text_train_3x, boosted_label_train_3x, \n",
    "                                      text_val, label_val,\n",
    "                                      cv_binary=True, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                      cv_max_features=5000, cv_have_tfidf=True, cv_use_idf=True, \n",
    "                                      cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)\n",
    "\n",
    "print(\"test:\")\n",
    "measure_model_on_test(cfr_pipeline_3x, text_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9852    0.9849    0.9851     10231\n",
      "           1     0.9856    0.9859    0.9857     10697\n",
      "\n",
      "    accuracy                         0.9854     20928\n",
      "   macro avg     0.9854    0.9854    0.9854     20928\n",
      "weighted avg     0.9854    0.9854    0.9854     20928\n",
      "\n",
      "[[10077   154]\n",
      " [  151 10546]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9662    0.9764    0.9713      2374\n",
      "           1     0.9767    0.9666    0.9716      2426\n",
      "\n",
      "    accuracy                         0.9715      4800\n",
      "   macro avg     0.9715    0.9715    0.9715      4800\n",
      "weighted avg     0.9715    0.9715    0.9715      4800\n",
      "\n",
      "[[2318   56]\n",
      " [  81 2345]]\n",
      "test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9678    0.9633    0.9655      4000\n",
      "           1     0.9634    0.9680    0.9657      4000\n",
      "\n",
      "    accuracy                         0.9656      8000\n",
      "   macro avg     0.9656    0.9656    0.9656      8000\n",
      "weighted avg     0.9656    0.9656    0.9656      8000\n",
      "\n",
      "[[3853  147]\n",
      " [ 128 3872]]\n"
     ]
    }
   ],
   "source": [
    "boosted_text_train_4x = pd.concat([text_train]+[extra_text_train]*4)\n",
    "boosted_label_train_4x = np.concatenate([label_train]+[extra_label_train]*4, axis=0)\n",
    "\n",
    "cfr_pipeline_4x = train_measure_model(boosted_text_train_4x, boosted_label_train_4x, \n",
    "                                      text_val, label_val,\n",
    "                                      cv_binary=True, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                      cv_max_features=5000, cv_have_tfidf=True, cv_use_idf=True, \n",
    "                                      cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)\n",
    "\n",
    "print(\"test:\")\n",
    "measure_model_on_test(cfr_pipeline_4x, text_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9892    0.9867    0.9879     10382\n",
      "           1     0.9875    0.9898    0.9886     10978\n",
      "\n",
      "    accuracy                         0.9883     21360\n",
      "   macro avg     0.9883    0.9883    0.9883     21360\n",
      "weighted avg     0.9883    0.9883    0.9883     21360\n",
      "\n",
      "[[10244   138]\n",
      " [  112 10866]]\n",
      "validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9658    0.9768    0.9713      2374\n",
      "           1     0.9771    0.9662    0.9716      2426\n",
      "\n",
      "    accuracy                         0.9715      4800\n",
      "   macro avg     0.9715    0.9715    0.9715      4800\n",
      "weighted avg     0.9715    0.9715    0.9715      4800\n",
      "\n",
      "[[2319   55]\n",
      " [  82 2344]]\n",
      "test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9698    0.9635    0.9666      4000\n",
      "           1     0.9637    0.9700    0.9669      4000\n",
      "\n",
      "    accuracy                         0.9667      8000\n",
      "   macro avg     0.9668    0.9667    0.9667      8000\n",
      "weighted avg     0.9668    0.9667    0.9667      8000\n",
      "\n",
      "[[3854  146]\n",
      " [ 120 3880]]\n"
     ]
    }
   ],
   "source": [
    "boosted_text_train_5x = pd.concat([text_train]+[extra_text_train]*5)\n",
    "boosted_label_train_5x = np.concatenate([label_train]+[extra_label_train]*5, axis=0)\n",
    "\n",
    "cfr_pipeline_5x = train_measure_model(boosted_text_train_5x, boosted_label_train_5x, \n",
    "                                      text_val, label_val,\n",
    "                                      cv_binary=True, cv_analyzer=\"word\", cv_ngram=(\"w\", 1, 3), \n",
    "                                      cv_max_features=5000, cv_have_tfidf=True, cv_use_idf=True, \n",
    "                                      cfr_penalty=\"l2\", cfr_C=1.0, stop_words=None)\n",
    "\n",
    "print(\"test:\")\n",
    "measure_model_on_test(cfr_pipeline_5x, text_test, label_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
